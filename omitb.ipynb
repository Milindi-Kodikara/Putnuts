{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Inside the heads of Putnuts\n",
    "#### An analysis of the Only Murders in the Building TV show subreddit\n",
    "\n",
    "Milindi Kodikara"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ffc24dbff90242e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step 1 : : Set up"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc7521a9bd0a65f2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from client import client\n",
    "import helper\n",
    "\n",
    "\n",
    "import praw\n",
    "import string\n",
    "import json\n",
    "import codecs\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "# TODO: Ask why use Tweet Tokenizer, why not word_tokenizer \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "from colorama import Fore, Back, Style\n",
    "import pandas as pd\n",
    "\n",
    "# misc\n",
    "import datetime as dt\n",
    "from pprint import pprint\n",
    "from itertools import chain\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 8) # default plot size\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid', palette='Dark2')\n",
    "from wordcloud import WordCloud"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce4026b269df7302",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "subreddit_name = 'OnlyMurdersHulu'\n",
    "# maximum number of hot submissions\n",
    "limit = 5"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5566865b12f59784",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "reddit_client = client()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27fec5e5d6f2e7d8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# sanity check\n",
    "print('Username :: ', reddit_client.user.me())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6dc561188ad2d632",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "subreddit = reddit_client.subreddit(subreddit_name)\n",
    "\n",
    "print('Subreddit :: ', subreddit)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5bedec8775778c98",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "posts = [*subreddit.hot(limit=limit)] \n",
    "\n",
    "len(posts)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc48f36a14f70818",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# for submission in posts:\n",
    "#     print('Title: {}\\nAuthor: {}\\nDescription: {}\\n---------'.format(submission.title, submission.author.name, submission.selftext))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74c72c986c21ad7d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step 2 : : Pre-processing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6bfec205fc14f7c8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokeniser = TweetTokenizer()\n",
    "stop_words = stopwords.words('english') + list(string.punctuation) + ['rt', 'via', '...', 'â€¦', '\"', \"'\", '`']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2cb4af43c0fcaa3c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Tokenise and pre-process -> Add to a new dataframe to be analysed \n",
    "token_list = []\n",
    "for submission in subreddit.hot(limit=limit):\n",
    "    post_description = submission.selftext\n",
    "    post_title = submission.title\n",
    "    \n",
    "    post_title_description = post_title + \" \" + post_description\n",
    "    \n",
    "    # text, tokeniser, stop_words\n",
    "    token_list.append(helper.process(post_title_description, tokeniser, stop_words))\n",
    "    \n",
    "    comment_token_list = []\n",
    "    submission.comments.replace_more()\n",
    "    for comment in submission.comments:\n",
    "        comment_text = comment.body\n",
    "        \n",
    "        comment_token_list.append(helper.process(comment_text, tokeniser, stop_words))\n",
    "    \n",
    "    print('Title: {}\\nAuthor: {}\\nDescription: {}\\n'.format(submission.title, submission.author.name, submission.selftext))\n",
    "    print('Token list for post: {}\\nfor comments: {} \\n-----------------\\n'.format(token_list, comment_token_list))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a240e413d8c55959",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step 3 : : Models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e127a46c0ba8b737"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# sentiment_analysis = SentimentIntensityAnalyzer()\n",
    "\n",
    "# sentiment_scores = sentiment_analysis.polarity_scores(\" \".join(token_list))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2010b2762c2d2a4b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# https://medium.com/bitgrit-data-science-publication/sentiment-analysis-on-reddit-tech-news-with-python-cbaddb8e9bb6"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "abfcb57cd428c089"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step 4 : : Analysis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8fd46fc6398415aa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
